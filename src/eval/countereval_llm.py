from typing import List
from pathlib import Path
import json
import re
import torch
import pandas as pd

from src.eval.load_model import load_countereval_model


METRIC_KEYS = [
    "feasibility",
    "consistency",
    "completeness",
    "trust",
    "understandability",
    "fairness",
    "complexity",
    "overall_satisfaction",
]

ALLOWED_VALUES = {"low", "medium", "high"}


class CounterEvalLLMEvaluator:
    def __init__(self, max_new_tokens: int = 64, batch_size: int = 2):
        self.model, self.tokenizer = load_countereval_model()
        self.max_new_tokens = max_new_tokens
        self.batch_size = batch_size

    def _build_prompt(self, cf_text: str) -> str:
        return f"""
You are evaluating counterfactual explanations generated by recourse algorithms. Counterfactual explanations explain what parameters of a situation should have been different for the outcome to have been different. You are not expected to provide reasoning or explanation and should answer with the appropriate value from the set [”low”, ”medium”, ”high”]. The definition of satisfaction: this scenario effectively explains how to reach a different outcome. The definition of feasibility: the actions suggested by the explanation are practical, realistic to implement and actionable. The definition of consistency: the parts of the explanation do not contradict each other. The definition of completeness: the explanation is sufficient in explaining how to achieve the desired outcome. The definition of trust: I believe that the suggested changes would bring about the desired outcome. The definition of understandability: I feel like I understood the phrasing of the explanation well. The definition of fairness: the explanation is unbiased towards different user groups and does not operate on sensitive features. The definition of complexity: the explanation has an appropriate level of detail and complexity - not too simple, yet not overly complex.

Respond with ONLY a single JSON object and nothing else. The JSON must have exactly the following keys: 
{{
    "feasibility": value, 
    "consistency": value, 
    "completeness": value, 
    "trust": value, 
    "understandability": value, 
    "fairness": value, 
    "complexity": value, 
    "overall_satisfaction": value 
}}

The following is the counterfactual explanation:

{cf_text}
""".strip()


    def _call_model(self, prompts: List[str]) -> List[str]:
        inputs = self.tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
        ).to(self.model.device)

        eos_token_id = self.tokenizer.eos_token_id
        if eos_token_id is None and "llama" in self.model.config.model_type:
            eos_token_id = self.tokenizer.convert_tokens_to_ids("</s>")

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.max_new_tokens * 2,
                do_sample=False,
                eos_token_id=eos_token_id,
                pad_token_id=eos_token_id,
            )

        gen_only = outputs[:, inputs["input_ids"].shape[1]:]
        decoded = self.tokenizer.batch_decode(gen_only, skip_special_tokens=True)

        return decoded

    def _parse_output(self, text: str) -> dict:
        # Find the last {...} block in the text
        match = None
        for m in re.finditer(r"\{.*?\}", text, flags=re.DOTALL):
            match = m

        if match is None:
            data = {}
        else:
            json_str = match.group(0)
            try:
                data = json.loads(json_str)
            except Exception:
                data = {}

        result = {}
        for k in METRIC_KEYS:
            v = data.get(k, None)

            # Normalize strings
            if isinstance(v, str):
                v_norm = v.strip().lower()
                if v_norm in ALLOWED_VALUES:
                    v = v_norm
                else:
                    v = None  # reject weird stuff
            else:
                v = None if v is not None else None

            result[k] = v

        return result

    def evaluate(
        self,
        df: pd.DataFrame,
        cf_col: str = "cf_story",
    ) -> pd.DataFrame:

        prompts = [self._build_prompt(c) for c in df[cf_col]]

        all_results = []
        for i in range(0, len(prompts), self.batch_size):
            batch = prompts[i : i + self.batch_size]
            raw = self._call_model(batch)
            parsed = [self._parse_output(t) for t in raw]
            all_results.extend(parsed)

        scores_df = pd.DataFrame(all_results)
        return pd.concat([df.reset_index(drop=True), scores_df], axis=1)